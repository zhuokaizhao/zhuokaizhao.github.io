---
title: "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment"
date: 2025-01-16
year: 2025
paper: True
venue: 'ArXiv'
arxiv: 2501.09620
arxiv_url: https://arxiv.org/pdf/2501.09620
code: https://github.com/alecwangcq/causal_reward
authors: <a target="_blank" href='https://alecwangcq.github.io/'>Chaoqi Wang*</a>,
         <b>Zhuokai Zhao*</b>,
         <a target="_blank" href='https://ybjiaang.github.io/'>Yibo Jiang*</a>,
         <a target="_blank" href='https://billchan226.github.io/'>Zhaorun Chen*</a>, 
         Chen Zhu,
         <a target="_blank" href='https://yuxinchen.org/'>Yuxin Chen</a>,
         Jiayi Liu,
         Lizhu Zhang,
         <a target="_blank" href='https://www.haoma.io/'>Hao Ma</a>, 
         and <a target="_blank" href='https://sites.google.com/site/snongwang/'>Sinong Wang</a>
publication: In submission
---
